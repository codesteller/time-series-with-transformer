{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Prediction with Tensorflow\n",
    "Goal of the model:\n",
    "*  Predict Global_active_power at a specified time in the future.\n",
    "*  Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "*  We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
    "\n",
    "Credit to [3 Steps to Forecast Time Series: LSTM with TensorFlow Keras](https://towardsdatascience.com/3-steps-to-forecast-time-series-lstm-with-tensorflow-keras-ba88c6f05237#:~:text=Long%20short-term%20memory%20(LSTM)%20is%20an%20artificial%20recurrent,events%20in%20a%20time%20series.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from datetime import timedelta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:2: DtypeWarning: Columns (2,3,4,5,6,7) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.87 s, sys: 133 ms, total: 2 s\n",
      "Wall time: 2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read the dataset into python\n",
    "df = pd.read_csv('./dataset/household_power_consumption.txt', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.740   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.680   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0           18.400          0.000          1.000            17.0  \n",
       "1           23.000          0.000          1.000            16.0  \n",
       "2           23.000          0.000          2.000            17.0  \n",
       "3           23.000          0.000          1.000            17.0  \n",
       "4           15.800          0.000          1.000            17.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Data\n",
    "Transform the dataset by:\n",
    "* creating feature date_time in DateTime format by combining Date and Time.\n",
    "* converting Global_active_power to numeric and remove missing values (1.25%).\n",
    "* ordering the features by time in the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after removing missing values: (2049280, 2)\n",
      "The time series starts from:  2006-12-16 17:24:00\n",
      "The time series ends on:  2010-12-11 23:59:00\n",
      "CPU times: user 2min 56s, sys: 549 ms, total: 2min 56s\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code is copied from https://towardsdatascience.com/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba\n",
    "# with a few minor changes.\n",
    "#\n",
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2049280 entries, 0 to 2049279\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Dtype         \n",
      "---  ------               -----         \n",
      " 0   date_time            datetime64[ns]\n",
      " 1   Global_active_power  float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 31.3 MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Global_active_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-16 17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-16 17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-16 17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-16 17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-12-16 17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-12-16 17:29:00</td>\n",
       "      <td>3.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-12-16 17:30:00</td>\n",
       "      <td>3.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-12-16 17:31:00</td>\n",
       "      <td>3.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-12-16 17:32:00</td>\n",
       "      <td>3.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-12-16 17:33:00</td>\n",
       "      <td>3.662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time  Global_active_power\n",
       "0 2006-12-16 17:24:00                4.216\n",
       "1 2006-12-16 17:25:00                5.360\n",
       "2 2006-12-16 17:26:00                5.374\n",
       "3 2006-12-16 17:27:00                5.388\n",
       "4 2006-12-16 17:28:00                3.666\n",
       "5 2006-12-16 17:29:00                3.520\n",
       "6 2006-12-16 17:30:00                3.702\n",
       "7 2006-12-16 17:31:00                3.700\n",
       "8 2006-12-16 17:32:00                3.668\n",
       "9 2006-12-16 17:33:00                3.662"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.info()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the Dataset\n",
    "Next, we split the dataset into training, validation, and test datasets.\n",
    "df_test holds the data within the last 7 days in the original dataset. df_val has data 14 days before the test dataset. df_train has the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dates: 2010-12-05 00:00:00 to 2010-12-11 23:59:00\n",
      "Validation dates: 2010-11-21 00:00:00 to 2010-12-04 23:59:00\n",
      "Train dates: 2006-12-16 17:24:00 to 2010-11-20 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the Dataset for TensorFlow Keras\n",
    "Dividing the Dataset into Smaller Dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of the model:\n",
    "#  Predict Global_active_power at a specified time in the future.\n",
    "#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
    "\n",
    "\n",
    "def create_ts_files(dataset, \n",
    "                    start_index, \n",
    "                    end_index, \n",
    "                    history_length, \n",
    "                    step_size, \n",
    "                    target_step, \n",
    "                    num_rows_per_file, \n",
    "                    data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_step\n",
    "    \n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
    "    \n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "            \n",
    "        # get the start and end indices.\n",
    "        ind0 = i*num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "        \n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j-1, j-history_length-1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j+target_step]]\n",
    "            \n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "            \n",
    "    return len(col_names)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create 158 files (each including a pandas dataframe) within the folder ts_data.\n",
    "* return num_timesteps as the number of lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 158 files.\n",
      "ts_data/ts_file0.pkl\n",
      "ts_data/ts_file10.pkl\n",
      "ts_data/ts_file20.pkl\n",
      "ts_data/ts_file30.pkl\n",
      "ts_data/ts_file40.pkl\n",
      "ts_data/ts_file50.pkl\n",
      "ts_data/ts_file60.pkl\n",
      "ts_data/ts_file70.pkl\n",
      "ts_data/ts_file80.pkl\n",
      "ts_data/ts_file90.pkl\n",
      "ts_data/ts_file100.pkl\n",
      "ts_data/ts_file110.pkl\n",
      "ts_data/ts_file120.pkl\n",
      "ts_data/ts_file130.pkl\n",
      "ts_data/ts_file140.pkl\n",
      "ts_data/ts_file150.pkl\n",
      "CPU times: user 9min 5s, sys: 29.4 s, total: 9min 34s\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "global_active_power = df_train['Global_active_power'].values\n",
    "\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_data')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Dataset\n",
    "Same as the training dataset, we also create a folder of the validation data, which prepares the validation dataset for model fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 files.\n",
      "ts_val_data/ts_file0.pkl\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation set.\n",
    "#\n",
    "# Create the validation CSV like we did before with the training.\n",
    "global_active_power_val = df_val['Global_active_power'].values\n",
    "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_val_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Time Series Object Class\n",
    "\n",
    "* init: the initial settings of the object, including:\n",
    "    - ts_folder, which will be ts_data that we just created.\n",
    "    - filename_format, which is the string format of the file names in the ts_folder.\n",
    "    For example, when the files are ts_file0.pkl, ts_file1.pkl, …, ts_file100.pkl, the format would be ‘ts_file{}.pkl’.\n",
    "* num_chunks: the total number of files (chunks).\n",
    "* get_chunk: this method takes the dataframe from one of the files, processes it to be ready for training.\n",
    "* shuffle_chunks: this method shuffles the order of the chunks that are returned in get_chunk. This is a good practice for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
    "# \n",
    "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "    def __init__(self, ts_folder, filename_format):\n",
    "        self.ts_folder = ts_folder\n",
    "        \n",
    "        # find the number of files.\n",
    "        i = 0\n",
    "        file_found = True\n",
    "        while file_found:\n",
    "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
    "            file_found = os.path.exists(filename)\n",
    "            if file_found:\n",
    "                i += 1\n",
    "                \n",
    "        self.num_files = i\n",
    "        self.files_indices = np.arange(self.num_files)\n",
    "        self.shuffle_chunks()\n",
    "        \n",
    "    def num_chunks(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def get_chunk(self, idx):\n",
    "        assert (idx >= 0) and (idx < self.num_files)\n",
    "        \n",
    "        ind = self.files_indices[idx]\n",
    "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    "        df_ts = pd.read_pickle(filename)\n",
    "        num_records = len(df_ts.index)\n",
    "        \n",
    "        features = df_ts.drop('y', axis=1).values\n",
    "        target = df_ts['y'].values\n",
    "        \n",
    "        # reshape for input into LSTM. Batch major format.\n",
    "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    "        return features_batchmajor, target\n",
    "    \n",
    "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
    "    def shuffle_chunks(self):\n",
    "        np.random.shuffle(self.files_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_folder = 'ts_data'\n",
    "filename_format = 'ts_file{}.pkl'\n",
    "tss = TimeSeriesLoader(ts_folder, filename_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras model.\n",
    "# Use hyperparameter optimization if you have the time.\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
    "\n",
    "# units=10 -> The cell and hidden states will be of dimension 10.\n",
    "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
    "x = layers.LSTM(units=10)(ts_inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1008, 1)]         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Specify the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model\n",
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0094 - mse: 0.0094\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0088 - mse: 0.0088\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0085 - mse: 0.0085: 0s - loss: 0.0082 -\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0128 - mse: 0.0128\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0122 - mse: 0.0122\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0076 - mse: 0.0076\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0087 - mse: 0.0087\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0078 - mse: 0.0078\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0085 - mse: 0.0085\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0080 - mse: 0.0080\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0079 - mse: 0.0079\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0068 - mse: 0.0068\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0028 - mse: 0.0028\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0075 - mse: 0.0075\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0077 - mse: 0.0077\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0086 - mse: 0.0086\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0064 - mse: 0.0064\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0082 - mse: 0.0082\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0077 - mse: 0.0077\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 32ms/step - loss: 0.0076 - mse: 0.0076\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0024 - mse: 0.0024\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0017 - mse: 0.0017\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 6.1876e-04 - mse: 6.1876e-04\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0062 - mse: 0.0062\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0081 - mse: 0.0081\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0080 - mse: 0.0080\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0070 - mse: 0.0070\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0022 - mse: 0.0022\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0063 - mse: 0.0063\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0021 - mse: 0.0021\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0081 - mse: 0.0081\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0084 - mse: 0.0084\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0024 - mse: 0.0024\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0056 - mse: 0.0056\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0064 - mse: 0.0064\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 3s 33ms/step - loss: 0.0038 - mse: 0.0038\n",
      "CPU times: user 9min 32s, sys: 1min 24s, total: 10min 56s\n",
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train in batch sizes of 128.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CHUNKS = tss.num_chunks()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for i in range(NUM_CHUNKS):\n",
    "        X, y = tss.get_chunk(i)\n",
    "        \n",
    "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
    "        # https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
    "        \n",
    "    # shuffle the chunks so they're not in the same order next time around.\n",
    "    tss.shuffle_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation mean squared error: 0.43265935673694833\n",
      "validation baseline mean squared error: 0.428345914375\n"
     ]
    }
   ],
   "source": [
    "# If we assume that the validation dataset can fit into memory we can do this.\n",
    "df_val_ts = pd.read_pickle('ts_val_data/ts_file0.pkl')\n",
    "\n",
    "\n",
    "features = df_val_ts.drop('y', axis=1).values\n",
    "features_arr = np.array(features)\n",
    "\n",
    "# reshape for input into LSTM. Batch major format.\n",
    "num_records = len(df_val_ts.index)\n",
    "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "y_act = df_val_ts['y'].values\n",
    "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
    "\n",
    "#baseline\n",
    "y_pred_baseline = df_val_ts['x_lag11'].values\n",
    "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
    "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
